no restart policy in containers
docker-compose up -d
python script needs kafka-python in kafka container
install using pip install kafka-python

-------------------

Kafka commands

kafka-topics --list --bootstrap-server localhost:9092
kafka-console-consumer  --bootstrap-server localhost:9092 --topic flink-response-topic

flink-response-topic
storm-request-topic
---------------------

Build and package storm

docker-compose up -d --build
docker-compose up -d
docker cp schema.cql storm-nimbus:/schema.cql
docker exec -it cassandra cqlsh -u cassandra -p cassandra -f /schema.cql
mvn clean package -> Create FAT jar
docker cp storm/test/target/test-1.0-SNAPSHOT.jar storm-nimbus:/wordcount.jar
docker cp notes.txt storm-supervisor:/tmp/notes.txt
docker exec -it storm-nimbus storm jar /wordcount.jar com.wordCount.Topology my-wordcount-topology

------------
flink
docker cp flink-agg/count_agg.py jobmanager:/opt/flink/count_agg.py
docker exec -it jobmanager ./bin/flink run -py /opt/flink/count_agg.py


docker exec -it cassandra cqlsh -e "SELECT * FROM storm_flink_ks.word_counts;"

Kill running topo
docker exec -it storm-nimbus storm kill my-wordcount-topology -c nimbus.seeds=["localhost"]


   checks
   javap -v target/classes/com/wordCount/Topology.class | grep major -> Issue in java version in container, check compiled version of jar
   mvn clean package -Dmaven.compiler.release=11 -> Forces v55

   Leader not found
   docker exec -it storm-nimbus storm jar /wordcount.jar com.wordCount.Topology my-wordcount-topology -c nimbus.seeds=["localhost"]
   docker exec -it storm-nimbus storm list -c nimbus.seeds=["localhost"] -> Check dns resolution
   docker exec -it storm-ui storm list -c nimbus.seeds=["storm-nimbus"] -> Check dns resolution
   docker logs storm-ui


=======================

Storm
   Spout: Takes input data and passes it
   Bolt: Takes data from spout and transforms, passes it to another bolt or persist in storage

   Components:
      Tuple: Each unit of data sent from spout to bolt will be a tuple. First column headers are sent and then the comma seperated values are sent using tuples
      Streams: Unbounded sequence of tuples
      Spout: Takes input data and passes it
      Bolt: Takes data from spout and transforms, passes it to another bolt or persist in storage
      Topologies: Visual representation of Spout and bolts (DAG).

   Storm running modes:
      Local mode: Runs on local machine, for testing
      Remote mode: Package topo to jar and submit to remote storm cluster


=========================

